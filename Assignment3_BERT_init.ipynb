{"cells":[{"cell_type":"code","execution_count":1,"id":"fd9c0199","metadata":{"id":"fd9c0199","executionInfo":{"status":"ok","timestamp":1701007844475,"user_tz":-60,"elapsed":7132,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["import pandas as pd\n","from transformers import BertTokenizer\n","import torch\n","from torch import nn\n","from transformers import BertModel\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYX7DmjTOJg0","executionInfo":{"status":"ok","timestamp":1701007846069,"user_tz":-60,"elapsed":1599,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"c5ba17ff-008a-4464-f240-140569f79022"},"id":"dYX7DmjTOJg0","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":3,"id":"dfcbd491","metadata":{"id":"dfcbd491","executionInfo":{"status":"ok","timestamp":1701007846069,"user_tz":-60,"elapsed":9,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["answers = pd.read_csv(\"drive/MyDrive/Levy/processed_data_english.csv\")"]},{"cell_type":"code","execution_count":144,"id":"a9ae9806","metadata":{"id":"a9ae9806","outputId":"8cac36ca-1d1d-4964-cb6b-bbf6162969c4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701016477449,"user_tz":-60,"elapsed":2,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2798"]},"metadata":{},"execution_count":144}],"source":["len(answers['TEXT'][4])"]},{"cell_type":"code","source":["answers['TEXT'][4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"Sc7cgzJWe1lr","executionInfo":{"status":"ok","timestamp":1701016483131,"user_tz":-60,"elapsed":5,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"005c7037-dac0-4683-9191-725f604f0ed4"},"id":"Sc7cgzJWe1lr","execution_count":145,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'well good old stream consciousness assignment feel like back freshman english class thats bad thing mind english teacher freshman year made assignment constantly mine always completely ridiculous like wow really hungry wish could taco bell really point except busy work psychology class though see reasoning behind assignment like letting mind free putting random thought writing could big help figuring psychological screwup well thats true dont want yall getting wrong idea today first day class really nut case people may think really normal kind gal actually texas guess normal dont like eat biscuit gravy breakfast country fried steak fried okra dinner connecticut dont even okra much le worship like kind vegetable goddess mind starting blankperformance pressure guess spot herei dont want bored youre reading ever get around reading well going stress yet youre probably going listen random incoherent babbling paragraph computer big old pain as smf sure theyve got bajillion computer unfortunately weve got bajillion student trying use time think ill spending quite late late night computer center get stuff done yippee thats college aboutlate night library yeah right point dont even know college probably shouldnt say anything though seeing going write another one thingys day topic college blah blah blah cant believe actually assignment day assigned talk dedication really cant believe high school procrastination middle name first name second semester free period day actual class didnt damn thing great unfortunately going work much harder get studying skill back par high school trip couldnt wait get hated school town everything except friend course family moved right graduation learned real quick worse place old town least back home friend boyfriend piece crap car knew moved friend life car nothing worked day thats though ready whole college thing austin seems like fun city might actually enjoy spending next four year yeah subject four year professor administrator give speech stuff always make sound like well college like year sorry plan graduating year whats problem people cant graduate year dont get offense yall reading took like year undergrad work trying knock trying figure well exactly minute started nifty little piece writing make sense point really sure fulfilled assignment like supposed analyze personal stream consciousness took mean regarding personality guess could say mind work mysterious way even essay seems illogically connected see pattern yeah went back tried read ive got give suggestion assignment make box writing box see whole line writing one time without scroll across real big pain sure big pain youre trying read unless course read see whole line dont know suggestion thanks taking time give opportunity get easy final grade writing assignment'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":145}]},{"cell_type":"code","execution_count":5,"id":"efa0fe1b","metadata":{"id":"efa0fe1b","outputId":"3ac77477-f532-429f-80be-2bb60b0bf16e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701007846069,"user_tz":-60,"elapsed":5,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    1532\n","1    1431\n","Name: cOPN, dtype: int64"]},"metadata":{},"execution_count":5}],"source":["answers['cOPN'].value_counts()"]},{"cell_type":"code","source":["answers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"-7CKylhbgEdv","executionInfo":{"status":"ok","timestamp":1701018337974,"user_tz":-60,"elapsed":6,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"c70ff68e-d223-40b9-c3e9-bcca7b5fbf91"},"id":"-7CKylhbgEdv","execution_count":147,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   TEXT  cEXT  cNEU  cAGR  \\\n","0     well right woke midday nap sort weird ever sin...     0     1     1   \n","1     well stream consciousness essay used thing lik...     0     0     1   \n","2     open keyboard button push thing finally worked...     0     1     0   \n","3     cant believe really happening pulse racing lik...     1     0     1   \n","4     well good old stream consciousness assignment ...     1     0     1   \n","...                                                 ...   ...   ...   ...   \n","2958  motivated day day basis need provide little fa...     1     0     0   \n","2959  son biggest part life without reckless person ...     1     1     0   \n","2960  kid grandkids keep motivated everyday inspire ...     1     0     1   \n","2961  biggest drive earn money retire beach schedule...     0     0     0   \n","2962  people never give cause life cruel strong enem...     1     1     0   \n","\n","      cCON  cOPN  \n","0        0     1  \n","1        0     0  \n","2        1     1  \n","3        1     0  \n","4        0     1  \n","...    ...   ...  \n","2958     1     1  \n","2959     0     0  \n","2960     1     0  \n","2961     0     0  \n","2962     0     0  \n","\n","[2963 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-e0d02110-2720-439d-ac5a-50fc12499667\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TEXT</th>\n","      <th>cEXT</th>\n","      <th>cNEU</th>\n","      <th>cAGR</th>\n","      <th>cCON</th>\n","      <th>cOPN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>well right woke midday nap sort weird ever sin...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>well stream consciousness essay used thing lik...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>open keyboard button push thing finally worked...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>cant believe really happening pulse racing lik...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>well good old stream consciousness assignment ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2958</th>\n","      <td>motivated day day basis need provide little fa...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2959</th>\n","      <td>son biggest part life without reckless person ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2960</th>\n","      <td>kid grandkids keep motivated everyday inspire ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2961</th>\n","      <td>biggest drive earn money retire beach schedule...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2962</th>\n","      <td>people never give cause life cruel strong enem...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2963 rows Ã— 6 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0d02110-2720-439d-ac5a-50fc12499667')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e0d02110-2720-439d-ac5a-50fc12499667 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e0d02110-2720-439d-ac5a-50fc12499667');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ee895deb-be5e-43fa-bac0-8ac9c03d9cb0\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee895deb-be5e-43fa-bac0-8ac9c03d9cb0')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ee895deb-be5e-43fa-bac0-8ac9c03d9cb0 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":147}]},{"cell_type":"code","execution_count":6,"id":"4b568b07","metadata":{"id":"4b568b07","executionInfo":{"status":"ok","timestamp":1701007846634,"user_tz":-60,"elapsed":569,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":148,"id":"9f395c1e","metadata":{"id":"9f395c1e","outputId":"2bb70796-d4e6-491f-fc37-1f4b812cf266","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701018370536,"user_tz":-60,"elapsed":27676,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]\n"]}],"source":["import torch\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","max_length = 512  # BERT's maximum length\n","\n","input_ids = []\n","attention_masks = []\n","token_lengths = []  # List to store token lengths\n","\n","for text in answers['TEXT']:\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,                       # Text to encode\n","                        add_special_tokens=True,    # Add '[CLS]' and '[SEP]'\n","                        max_length=max_length,      # Pad & truncate\n","                        padding='max_length',       # Pad all sequences to the same length\n","                        return_attention_mask=True, # Construct attention masks\n","                        return_tensors='pt',        # Return PyTorch tensors\n","                        truncation=True             # Ensure truncation to max_length\n","                   )\n","\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    token_lengths.append(len(encoded_dict['input_ids'][0]))  # Store the token length\n","\n","# Convert lists to tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","# Now you can check the token lengths\n","print(token_lengths)\n"]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","input_ids = []\n","attention_masks = []\n","token_lengths = []  # List to store actual token lengths\n","\n","for text in answers['TEXT']:\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,                       # Text to encode\n","                        add_special_tokens=True,    # Add '[CLS]' and '[SEP]'\n","                        return_attention_mask=True, # Construct attention masks\n","                        return_tensors='pt',        # Return PyTorch tensors\n","                        truncation=True             # Truncate to max_length if necessary\n","                   )\n","\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    token_lengths.append(len(encoded_dict['input_ids'][0]))  # Store the actual token length\n","\n","# Convert lists to tensors (if necessary for your application)\n","# Be aware that without padding, these tensors may be of different lengths\n","# input_ids = torch.cat(input_ids, dim=0)\n","# attention_masks = torch.cat(attention_masks, dim=0)\n","\n","# Now you can check the actual token lengths\n","print(token_lengths)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIwwbmQ0mg68","executionInfo":{"status":"ok","timestamp":1701018512587,"user_tz":-60,"elapsed":30546,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"68638bb6-5900-4ef9-e59a-6bc255ea2435"},"id":"YIwwbmQ0mg68","execution_count":149,"outputs":[{"output_type":"stream","name":"stdout","text":["[294, 336, 428, 187, 491, 414, 328, 246, 255, 247, 253, 397, 410, 512, 169, 82, 328, 269, 164, 385, 286, 351, 229, 337, 395, 229, 136, 413, 472, 364, 223, 282, 303, 205, 160, 234, 225, 374, 237, 335, 164, 286, 301, 265, 288, 224, 306, 196, 242, 512, 376, 353, 254, 208, 453, 165, 240, 326, 293, 340, 250, 369, 178, 245, 229, 328, 230, 204, 180, 289, 263, 304, 114, 512, 401, 214, 184, 401, 161, 244, 122, 140, 406, 192, 251, 258, 392, 201, 344, 199, 247, 193, 512, 273, 59, 258, 440, 216, 338, 387, 241, 253, 104, 226, 314, 14, 300, 169, 327, 316, 385, 147, 301, 233, 334, 187, 224, 214, 234, 295, 451, 484, 217, 316, 242, 174, 285, 326, 324, 121, 255, 393, 117, 213, 218, 247, 178, 280, 136, 367, 230, 487, 264, 467, 403, 149, 168, 317, 214, 415, 180, 255, 316, 223, 373, 137, 280, 512, 512, 231, 178, 157, 222, 305, 418, 115, 328, 306, 259, 84, 168, 175, 164, 138, 357, 261, 399, 320, 381, 231, 131, 239, 409, 360, 473, 512, 327, 234, 324, 200, 512, 77, 203, 235, 339, 334, 348, 437, 267, 126, 254, 176, 312, 223, 278, 169, 290, 432, 335, 267, 232, 77, 442, 169, 512, 153, 194, 349, 225, 512, 322, 442, 445, 512, 217, 298, 240, 221, 308, 302, 246, 512, 175, 257, 433, 254, 248, 397, 309, 207, 332, 496, 342, 287, 395, 442, 301, 254, 124, 290, 125, 227, 188, 244, 204, 268, 425, 295, 290, 469, 457, 256, 255, 469, 236, 231, 82, 512, 407, 413, 150, 355, 126, 184, 304, 421, 150, 296, 128, 260, 351, 185, 159, 172, 255, 160, 80, 212, 198, 393, 218, 219, 211, 512, 237, 157, 207, 185, 437, 264, 193, 308, 211, 512, 424, 243, 410, 257, 338, 170, 217, 315, 379, 417, 213, 410, 380, 159, 130, 338, 336, 278, 390, 262, 341, 325, 274, 161, 252, 393, 472, 256, 315, 367, 266, 309, 230, 356, 389, 247, 286, 202, 306, 84, 171, 205, 31, 512, 131, 137, 512, 106, 362, 50, 99, 333, 218, 217, 151, 483, 150, 111, 134, 183, 351, 370, 380, 236, 479, 482, 455, 451, 512, 471, 316, 182, 146, 302, 428, 512, 199, 401, 384, 140, 193, 405, 149, 187, 95, 461, 287, 327, 73, 306, 160, 98, 383, 197, 327, 282, 292, 140, 289, 235, 103, 512, 143, 203, 247, 351, 195, 268, 243, 58, 512, 314, 407, 280, 181, 174, 292, 243, 148, 138, 170, 299, 391, 115, 278, 220, 260, 405, 221, 486, 151, 364, 414, 512, 512, 336, 357, 308, 257, 226, 236, 153, 407, 263, 200, 100, 291, 355, 343, 414, 148, 258, 155, 253, 261, 409, 380, 238, 328, 263, 292, 243, 257, 90, 111, 284, 261, 189, 167, 289, 117, 138, 358, 233, 295, 30, 180, 263, 144, 110, 383, 291, 218, 196, 319, 512, 489, 115, 168, 320, 241, 236, 271, 363, 299, 345, 298, 83, 166, 242, 221, 337, 376, 395, 512, 154, 175, 239, 316, 273, 471, 236, 110, 184, 372, 328, 202, 234, 311, 280, 241, 214, 435, 148, 455, 329, 293, 250, 256, 230, 258, 331, 291, 209, 229, 239, 317, 308, 225, 190, 152, 445, 104, 92, 166, 207, 249, 233, 344, 212, 139, 245, 264, 213, 459, 164, 374, 292, 454, 349, 310, 140, 288, 176, 230, 197, 181, 181, 90, 137, 282, 210, 337, 113, 372, 381, 163, 311, 322, 234, 211, 173, 202, 351, 301, 4, 335, 200, 225, 280, 369, 512, 327, 259, 385, 188, 346, 99, 199, 512, 359, 312, 252, 161, 408, 176, 220, 205, 180, 152, 81, 336, 356, 324, 465, 107, 247, 215, 512, 512, 176, 322, 386, 226, 324, 187, 132, 126, 206, 83, 257, 254, 221, 190, 313, 237, 201, 164, 254, 319, 306, 470, 445, 387, 354, 473, 222, 498, 495, 410, 284, 382, 392, 272, 351, 384, 132, 324, 397, 306, 255, 236, 320, 270, 423, 133, 200, 305, 208, 387, 353, 235, 182, 430, 206, 218, 323, 266, 375, 459, 414, 227, 362, 442, 346, 394, 347, 155, 205, 247, 358, 185, 358, 141, 421, 392, 170, 316, 178, 364, 104, 164, 281, 414, 383, 250, 216, 278, 227, 342, 227, 257, 235, 225, 407, 227, 227, 494, 165, 130, 277, 277, 265, 327, 475, 512, 512, 294, 289, 512, 262, 348, 512, 380, 109, 260, 446, 218, 232, 348, 168, 217, 251, 324, 211, 154, 214, 137, 447, 461, 427, 500, 512, 239, 290, 512, 315, 189, 197, 360, 374, 352, 376, 181, 242, 212, 214, 288, 201, 487, 302, 375, 474, 354, 344, 383, 253, 387, 350, 357, 127, 219, 274, 179, 152, 282, 274, 255, 105, 401, 230, 488, 209, 222, 283, 420, 335, 210, 413, 137, 512, 120, 219, 330, 319, 376, 187, 268, 328, 167, 394, 368, 245, 230, 179, 512, 227, 368, 454, 345, 487, 409, 456, 278, 471, 308, 342, 360, 237, 255, 384, 296, 133, 222, 349, 132, 487, 208, 376, 344, 257, 455, 242, 122, 198, 466, 275, 479, 311, 265, 72, 512, 311, 304, 408, 228, 239, 223, 425, 264, 278, 396, 352, 279, 280, 206, 512, 233, 420, 293, 331, 286, 163, 224, 136, 394, 512, 290, 512, 262, 275, 436, 369, 298, 427, 199, 284, 358, 217, 187, 191, 199, 192, 311, 191, 329, 118, 190, 255, 244, 475, 263, 144, 352, 315, 306, 169, 179, 116, 198, 177, 299, 500, 362, 397, 94, 333, 476, 512, 454, 300, 412, 292, 244, 303, 326, 422, 356, 230, 240, 145, 274, 512, 283, 474, 315, 249, 123, 392, 296, 174, 512, 268, 127, 284, 454, 458, 512, 399, 169, 374, 438, 222, 259, 289, 313, 287, 376, 323, 337, 146, 243, 231, 512, 301, 187, 512, 312, 241, 256, 175, 283, 271, 244, 348, 407, 170, 171, 451, 268, 227, 276, 251, 388, 486, 250, 383, 185, 294, 323, 512, 358, 243, 241, 507, 277, 400, 170, 247, 278, 195, 337, 512, 154, 252, 512, 512, 383, 276, 512, 296, 436, 493, 186, 212, 416, 178, 246, 443, 180, 270, 294, 297, 425, 132, 247, 226, 118, 149, 306, 309, 305, 184, 322, 230, 253, 126, 294, 277, 81, 179, 226, 318, 274, 481, 223, 383, 303, 358, 239, 425, 450, 493, 161, 385, 272, 378, 218, 415, 509, 414, 178, 233, 354, 200, 266, 512, 183, 405, 512, 483, 139, 451, 201, 307, 436, 242, 242, 418, 269, 147, 424, 102, 234, 336, 206, 296, 370, 259, 183, 184, 359, 224, 139, 270, 310, 220, 480, 291, 113, 288, 291, 334, 292, 339, 157, 217, 265, 273, 386, 317, 176, 221, 512, 316, 89, 507, 321, 347, 177, 375, 298, 396, 133, 210, 192, 144, 258, 482, 284, 439, 490, 374, 227, 474, 286, 412, 331, 390, 310, 303, 231, 262, 190, 170, 308, 512, 100, 378, 122, 468, 253, 326, 353, 322, 510, 295, 247, 217, 303, 240, 425, 225, 304, 253, 327, 220, 221, 261, 228, 250, 306, 272, 266, 307, 378, 258, 309, 382, 346, 330, 350, 228, 178, 314, 168, 318, 369, 454, 408, 310, 210, 177, 277, 353, 121, 333, 164, 145, 265, 396, 127, 278, 196, 223, 443, 134, 445, 224, 278, 269, 268, 279, 248, 313, 235, 370, 351, 413, 190, 187, 287, 321, 233, 321, 493, 396, 366, 452, 337, 476, 216, 211, 248, 512, 301, 356, 172, 330, 135, 287, 155, 512, 329, 106, 155, 231, 319, 292, 304, 341, 179, 406, 450, 187, 228, 232, 398, 431, 234, 246, 187, 273, 282, 191, 479, 187, 283, 248, 251, 298, 178, 192, 414, 259, 316, 330, 227, 512, 180, 138, 127, 221, 258, 420, 243, 512, 141, 315, 235, 189, 291, 309, 181, 292, 233, 141, 258, 379, 338, 215, 470, 489, 301, 411, 512, 336, 313, 417, 192, 156, 258, 396, 268, 294, 328, 258, 465, 250, 259, 512, 296, 310, 331, 512, 486, 382, 344, 291, 187, 175, 256, 248, 310, 188, 332, 261, 136, 293, 203, 204, 488, 447, 84, 246, 391, 235, 452, 512, 250, 512, 299, 364, 358, 512, 295, 176, 316, 241, 347, 301, 512, 203, 346, 289, 227, 418, 285, 285, 256, 247, 375, 394, 301, 359, 358, 273, 315, 265, 405, 107, 276, 314, 209, 219, 159, 291, 354, 116, 446, 209, 494, 324, 137, 135, 343, 198, 390, 147, 251, 218, 235, 342, 512, 190, 322, 157, 512, 161, 181, 169, 189, 185, 169, 139, 265, 179, 161, 234, 266, 512, 166, 150, 392, 349, 160, 438, 173, 95, 136, 471, 233, 435, 174, 340, 509, 342, 153, 258, 366, 351, 339, 500, 512, 224, 370, 292, 322, 212, 227, 169, 182, 227, 219, 122, 189, 211, 276, 255, 219, 377, 320, 388, 159, 340, 241, 382, 172, 145, 145, 355, 512, 246, 128, 229, 275, 205, 225, 153, 454, 447, 303, 142, 416, 476, 337, 231, 175, 288, 425, 512, 308, 191, 281, 313, 221, 294, 169, 234, 189, 313, 512, 354, 381, 512, 263, 498, 195, 512, 474, 512, 289, 284, 242, 165, 461, 501, 311, 277, 197, 395, 365, 229, 346, 340, 160, 439, 415, 356, 262, 363, 378, 282, 396, 256, 408, 512, 418, 251, 512, 512, 512, 318, 390, 332, 478, 485, 512, 446, 512, 464, 426, 512, 186, 111, 512, 323, 234, 439, 260, 405, 249, 397, 512, 335, 512, 288, 512, 176, 469, 298, 301, 185, 431, 448, 232, 241, 406, 472, 512, 268, 317, 303, 455, 376, 425, 341, 346, 350, 248, 344, 295, 385, 306, 492, 272, 512, 354, 490, 459, 152, 496, 512, 339, 336, 149, 352, 512, 348, 472, 512, 512, 512, 509, 370, 468, 427, 225, 423, 411, 479, 219, 417, 319, 281, 385, 405, 430, 353, 297, 512, 512, 323, 512, 406, 183, 305, 312, 378, 512, 266, 342, 245, 275, 512, 446, 208, 285, 327, 471, 391, 512, 320, 334, 215, 262, 261, 512, 512, 460, 483, 512, 339, 512, 261, 512, 458, 462, 399, 312, 418, 361, 333, 317, 512, 314, 372, 282, 381, 338, 512, 512, 429, 290, 363, 440, 277, 446, 512, 294, 408, 206, 275, 273, 399, 156, 512, 296, 400, 403, 241, 384, 432, 295, 345, 286, 280, 367, 391, 370, 280, 321, 502, 503, 366, 342, 396, 149, 278, 400, 244, 423, 400, 192, 142, 246, 478, 220, 386, 231, 229, 402, 512, 389, 144, 286, 446, 283, 510, 476, 199, 486, 252, 512, 512, 270, 288, 236, 323, 292, 512, 225, 488, 300, 289, 343, 255, 512, 329, 264, 303, 235, 155, 306, 277, 302, 288, 336, 404, 315, 426, 248, 431, 284, 480, 425, 375, 310, 464, 258, 441, 512, 161, 272, 240, 297, 440, 512, 246, 379, 459, 512, 199, 129, 400, 296, 314, 253, 211, 414, 300, 397, 310, 303, 438, 490, 296, 348, 288, 512, 396, 435, 397, 487, 456, 243, 350, 318, 227, 415, 451, 496, 417, 326, 512, 390, 198, 270, 398, 320, 285, 386, 432, 193, 431, 339, 311, 210, 269, 512, 477, 442, 367, 353, 207, 387, 215, 488, 311, 264, 356, 423, 284, 388, 489, 512, 477, 326, 440, 498, 512, 348, 340, 215, 387, 232, 408, 349, 202, 404, 140, 221, 436, 377, 487, 502, 317, 276, 224, 308, 341, 262, 243, 505, 512, 175, 372, 364, 445, 512, 512, 215, 373, 374, 477, 512, 165, 154, 506, 371, 304, 400, 460, 512, 312, 477, 252, 349, 512, 246, 443, 479, 298, 359, 512, 235, 408, 317, 386, 346, 356, 512, 368, 266, 396, 381, 388, 472, 410, 299, 179, 431, 359, 278, 304, 437, 464, 345, 492, 512, 327, 217, 275, 359, 411, 290, 326, 400, 459, 347, 327, 393, 296, 430, 403, 461, 512, 411, 394, 483, 391, 224, 427, 317, 512, 237, 281, 503, 209, 287, 352, 372, 466, 512, 167, 303, 191, 331, 258, 408, 252, 391, 320, 293, 322, 380, 295, 403, 399, 355, 383, 441, 316, 483, 279, 512, 512, 321, 512, 386, 403, 246, 461, 314, 310, 294, 335, 367, 303, 247, 317, 380, 491, 512, 293, 442, 419, 445, 411, 367, 499, 375, 362, 512, 512, 282, 458, 387, 341, 213, 338, 357, 441, 397, 512, 512, 366, 512, 411, 196, 394, 347, 314, 314, 480, 268, 278, 142, 463, 322, 343, 271, 250, 286, 311, 345, 228, 419, 512, 242, 394, 415, 366, 280, 394, 457, 175, 174, 403, 512, 238, 512, 355, 476, 478, 248, 231, 472, 512, 222, 512, 447, 512, 269, 284, 349, 432, 512, 413, 444, 443, 471, 410, 470, 401, 154, 512, 228, 373, 405, 301, 264, 366, 446, 479, 412, 451, 285, 512, 512, 292, 372, 450, 185, 477, 264, 233, 494, 352, 362, 345, 343, 318, 512, 250, 462, 492, 511, 428, 489, 474, 179, 394, 266, 227, 376, 297, 512, 416, 186, 270, 234, 432, 387, 483, 229, 354, 354, 304, 491, 393, 456, 272, 286, 289, 506, 342, 393, 315, 512, 504, 512, 478, 512, 512, 221, 301, 350, 176, 309, 355, 512, 265, 214, 450, 441, 450, 278, 512, 512, 437, 432, 482, 330, 190, 470, 329, 131, 341, 285, 338, 288, 512, 383, 240, 366, 296, 263, 325, 409, 286, 366, 512, 327, 433, 406, 213, 500, 360, 480, 372, 440, 257, 459, 378, 512, 397, 319, 167, 366, 324, 369, 296, 307, 441, 130, 437, 397, 157, 372, 335, 377, 309, 258, 360, 432, 512, 387, 238, 441, 456, 299, 229, 283, 287, 512, 419, 273, 383, 491, 139, 360, 177, 293, 500, 277, 512, 387, 317, 296, 512, 322, 443, 206, 367, 401, 351, 146, 286, 376, 285, 468, 380, 377, 412, 338, 343, 422, 512, 354, 432, 276, 467, 283, 290, 512, 383, 347, 512, 216, 279, 202, 295, 360, 512, 305, 482, 407, 379, 512, 421, 347, 491, 512, 278, 313, 334, 363, 512, 347, 368, 385, 289, 404, 398, 379, 225, 331, 156, 231, 298, 383, 244, 500, 355, 437, 364, 406, 512, 269, 189, 400, 400, 427, 425, 268, 247, 329, 162, 477, 512, 375, 512, 242, 512, 512, 503, 298, 463, 307, 484, 244, 512, 221, 410, 432, 512, 259, 494, 342, 444, 93, 512, 464, 249, 320, 354, 431, 426, 512, 182, 512, 256, 259, 512, 393, 320, 435, 262, 440, 382, 303, 457, 512, 254, 512, 512, 405, 387, 512, 472, 282, 313, 512, 418, 496, 512, 188, 361, 281, 496, 269, 452, 512, 306, 512, 403, 360, 223, 355, 493, 507, 490, 512, 512, 299, 291, 346, 295, 388, 440, 428, 216, 347, 264, 354, 509, 300, 236, 512, 338, 326, 213, 350, 300, 279, 512, 400, 491, 338, 512, 176, 449, 299, 426, 278, 308, 411, 512, 179, 302, 300, 411, 387, 512, 385, 397, 412, 333, 147, 434, 194, 421, 512, 401, 227, 378, 387, 189, 512, 271, 146, 236, 240, 382, 449, 267, 266, 164, 347, 279, 431, 187, 292, 284, 202, 235, 69, 227, 176, 183, 193, 222, 195, 267, 349, 301, 221, 276, 512, 237, 217, 269, 166, 193, 240, 185, 250, 359, 187, 512, 387, 229, 288, 303, 329, 282, 227, 388, 182, 205, 215, 382, 308, 189, 187, 166, 302, 287, 212, 332, 203, 512, 161, 413, 273, 258, 218, 222, 161, 284, 301, 162, 187, 171, 172, 134, 172, 186, 176, 216, 272, 201, 144, 232, 168, 213, 233, 464, 184, 176, 258, 188, 203, 304, 178, 253, 176, 260, 217, 218, 258, 170, 195, 253, 222, 327, 194, 169, 354, 374, 170, 264, 336, 168, 201, 306, 266, 196, 218, 176, 226, 217, 225, 315, 217, 238, 223, 229, 230, 166, 244, 171, 152, 222, 256, 158, 170, 203, 300, 262, 179, 164, 152, 209, 255, 183, 169, 208, 165, 191, 512, 267, 474, 204, 216, 270, 213, 153, 218, 202, 263, 204, 240, 228, 214, 176, 175, 287, 227, 512, 276, 240, 348, 397, 240, 368, 185, 162, 325, 207, 241, 512, 342, 289, 198, 204, 191, 168, 240, 205, 215, 371, 182, 210, 150, 258, 272, 220, 488, 175, 173, 187, 190, 173, 298, 306, 172, 311, 171, 237, 252, 201, 189, 229, 165, 189, 203, 159, 251, 254, 229, 180, 179, 380, 112, 156, 216, 193, 154, 361, 171, 196, 293, 189, 141, 199, 206, 164, 250, 276, 127, 310, 180, 157, 276, 203, 186, 177, 285, 175, 298, 272, 512, 220, 188, 197, 167, 162, 170, 213, 189, 182, 337, 208, 265, 235, 182, 177, 488, 173, 347, 184, 368, 183, 284, 154, 193, 176, 233, 298, 293, 290, 315, 189, 212, 193, 438, 226, 176, 100, 241, 311, 262, 475, 372, 213, 159, 244, 202, 202, 220, 372, 166, 187, 512, 164, 168, 439, 134, 218, 512, 207, 255, 292, 260, 226, 251, 412, 225, 360, 261, 351, 188, 138, 173, 207, 252, 178, 235, 318, 208, 242, 242, 154, 416, 233, 255, 512, 259, 199, 512, 186, 250, 269, 180, 312, 335, 194, 199, 223, 256, 323, 256, 512, 158, 229, 206, 331, 300, 331, 226, 184, 168, 230, 512, 167, 320, 406, 172, 291, 232, 216, 421, 187, 186, 240, 303, 294, 419, 348, 291, 176, 230, 290, 304, 193, 353, 261, 164, 219, 164, 168, 225, 512, 269, 196, 512, 260, 249, 187, 167, 304, 200, 233, 203, 313, 158, 430, 289, 298, 216, 313, 273, 165, 465, 198, 300, 168, 281, 227, 311, 351, 222, 512, 227, 302, 217, 323, 168, 345, 227, 163, 172, 253, 352, 233, 212, 167, 298, 284, 200, 272, 273, 334, 305, 442, 210, 272, 274, 222, 223, 239, 214, 253, 222, 291, 196, 273, 207, 318, 253, 221, 164, 281, 272, 262, 298, 265, 285, 207, 164, 208, 214, 167, 254, 183, 272, 184, 176, 222, 201, 265, 220, 207, 379, 149, 206, 278]\n"]}]},{"cell_type":"code","source":["input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haC8PySCNSmQ","executionInfo":{"status":"ok","timestamp":1701016224647,"user_tz":-60,"elapsed":335,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"1e9810c6-7e2d-4000-af4f-141136e3a9d2"},"id":"haC8PySCNSmQ","execution_count":135,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 101, 2092, 2157,  ...,    0,    0,    0],\n","        [ 101, 2092, 5460,  ...,    0,    0,    0],\n","        [ 101, 2330, 9019,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 4845, 2882,  ...,    0,    0,    0],\n","        [ 101, 5221, 3298,  ...,    0,    0,    0],\n","        [ 101, 2111, 2196,  ...,    0,    0,    0]])"]},"metadata":{},"execution_count":135}]},{"cell_type":"code","execution_count":80,"id":"68304ffc","metadata":{"id":"68304ffc","executionInfo":{"status":"ok","timestamp":1701011989126,"user_tz":-60,"elapsed":331,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Combine input_ids and attention_masks into a single dataset\n","combined_data = list(zip(input_ids, attention_masks))\n","\n","# Splitting the combined data along with labels\n","train_data, test_data, y_train_ext, y_test_ext = train_test_split(combined_data, answers['cEXT'],\n","                                                                 random_state=42, test_size=0.2)\n","\n","# Separate input_ids and attention_masks again after splitting\n","X_train_ext, train_masks_ext = zip(*train_data)\n","X_test_ext, test_masks_ext = zip(*test_data)\n","\n","# Convert them back to tensors\n","X_train_ext = torch.stack(X_train_ext)\n","train_masks_ext = torch.stack(train_masks_ext)\n","X_test_ext = torch.stack(X_test_ext)\n","test_masks_ext = torch.stack(test_masks_ext)\n"]},{"cell_type":"code","source":["X_train_ext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsey_m0dNFXi","executionInfo":{"status":"ok","timestamp":1701011995002,"user_tz":-60,"elapsed":673,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"68ad0034-94a2-456e-a529-de7d2c7fcf10"},"id":"rsey_m0dNFXi","execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  101,  2514, 13394,  ...,     0,     0,     0],\n","        [  101,  2215,  2156,  ...,     0,     0,     0],\n","        [  101,  2521,  2465,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  5460,  8298,  ...,     0,     0,     0],\n","        [  101,  2183,  2132,  ...,     0,     0,     0],\n","        [  101,  2288,  2067,  ...,     0,     0,     0]])"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["y_train_ext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Evf4O6dp0t9","executionInfo":{"status":"ok","timestamp":1701011998964,"user_tz":-60,"elapsed":2,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"8a5412be-7ff0-435f-c432-e0690f33d093"},"id":"_Evf4O6dp0t9","execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["518     0\n","2778    0\n","573     0\n","2810    1\n","2390    0\n","       ..\n","1638    1\n","1095    1\n","1130    1\n","1294    1\n","860     0\n","Name: cEXT, Length: 2370, dtype: int64"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","execution_count":83,"id":"09aec218","metadata":{"id":"09aec218","executionInfo":{"status":"ok","timestamp":1701012085030,"user_tz":-60,"elapsed":1063,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["y_train_ext = y_train_ext.reset_index(drop=True)\n","y_test_ext = y_test_ext.reset_index(drop=True)"]},{"cell_type":"code","execution_count":84,"id":"41d239a7","metadata":{"id":"41d239a7","executionInfo":{"status":"ok","timestamp":1701012086275,"user_tz":-60,"elapsed":2,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["y_train_ext = torch.tensor(y_train_ext)\n","y_test_ext = torch.tensor(y_test_ext)"]},{"cell_type":"code","source":["y_train_ext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZvpuAATo4-G","executionInfo":{"status":"ok","timestamp":1701012093422,"user_tz":-60,"elapsed":3,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"46033667-9880-4177-c5fd-5e96d3fec6c1"},"id":"NZvpuAATo4-G","execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0,  ..., 1, 1, 0])"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","execution_count":87,"id":"712be116","metadata":{"id":"712be116","outputId":"4f9e5804-b779-4bd4-968a-7ee836e18aaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701012103924,"user_tz":-60,"elapsed":353,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2370"]},"metadata":{},"execution_count":87}],"source":["len(y_train_ext)"]},{"cell_type":"code","execution_count":88,"id":"014912f5","metadata":{"id":"014912f5","executionInfo":{"status":"ok","timestamp":1701012112433,"user_tz":-60,"elapsed":2,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 8  # You can adjust batch size\n","\n","# Create the DataLoader for training set\n","train_data = TensorDataset(X_train_ext, train_masks_ext, y_train_ext)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for test set\n","test_data = TensorDataset(X_test_ext, test_masks_ext, y_test_ext)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":89,"id":"cb09c772","metadata":{"id":"cb09c772","executionInfo":{"status":"ok","timestamp":1701012130453,"user_tz":-60,"elapsed":284,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["# class BertBinaryClassifier(nn.Module):\n","#     def __init__(self, freeze_bert=False):\n","#         super(BertBinaryClassifier, self).__init__()\n","\n","#         # Use the 'bert-base-uncased' pre-trained BERT model\n","#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","#         # Enhanced classifier layers\n","#         self.classifier = nn.Sequential(\n","#             nn.Dropout(0.1),  # Dropout for regularization\n","#             nn.Linear(768, 512),  # First layer\n","#             nn.ReLU(),  # Activation function\n","#             nn.Dropout(0.1),  # Additional dropout layer for regularization\n","#             nn.Linear(512, 128),  # Second layer\n","#             nn.ReLU(),  # Activation function\n","#             nn.Linear(128, 2)  # Final layer for binary classification\n","#         )\n","\n","#         if freeze_bert:\n","#             for param in self.bert.parameters():\n","#                 param.requires_grad = False\n","\n","#     def forward(self, input_ids, attention_mask):\n","#         # Pass the inputs through BERT\n","#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","\n","#         # Extract the last hidden state of the token `[CLS]` for classification\n","#         cls_output = outputs.pooler_output\n","\n","#         # Pass the BERT output through the classifier\n","#         logits = self.classifier(cls_output)\n","\n","#         return logits\n"]},{"cell_type":"code","source":["class SimplifiedBertClassifier(nn.Module):\n","    def __init__(self, freeze_bert=False):\n","        super(SimplifiedBertClassifier, self).__init__()\n","\n","        # Use the 'bert-base-uncased' pre-trained BERT model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Simplified classifier layer\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.1),  # Dropout for regularization\n","            nn.Linear(768, 2)  # Directly mapping BERT output to two classes\n","        )\n","\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Pass the inputs through BERT\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        # Extract the last hidden state of the token `[CLS]` for classification\n","        cls_output = outputs.pooler_output\n","\n","        # Pass the BERT output through the classifier\n","        logits = self.classifier(cls_output)\n","\n","        return logits\n"],"metadata":{"id":"quULNX9bz8jW","executionInfo":{"status":"ok","timestamp":1701014336951,"user_tz":-60,"elapsed":353,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"quULNX9bz8jW","execution_count":115,"outputs":[]},{"cell_type":"code","execution_count":116,"id":"8b3ffcde","metadata":{"id":"8b3ffcde","executionInfo":{"status":"ok","timestamp":1701014341021,"user_tz":-60,"elapsed":1883,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["model = SimplifiedBertClassifier()\n"]},{"cell_type":"code","source":["# from transformers import BertForSequenceClassification\n","\n","# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"],"metadata":{"id":"MQ6tBNAT8tqG","executionInfo":{"status":"ok","timestamp":1701014324971,"user_tz":-60,"elapsed":258,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"MQ6tBNAT8tqG","execution_count":114,"outputs":[]},{"cell_type":"code","execution_count":117,"id":"da0984ff","metadata":{"id":"da0984ff","executionInfo":{"status":"ok","timestamp":1701014376131,"user_tz":-60,"elapsed":363,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["from torch.optim import Adam\n","\n","# Define the optimizer\n","optimizer = Adam(model.parameters(), lr=1e-6)  # Learning rate is adjustable\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n"]},{"cell_type":"code","execution_count":118,"id":"a5a95f8f","metadata":{"id":"a5a95f8f","outputId":"c34eb36a-2c3c-4a1b-f74a-c336b17e24a8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701014379179,"user_tz":-60,"elapsed":424,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SimplifiedBertClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.1, inplace=False)\n","    (1): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":118}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n"]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4qGCN4Z1O_Xb","executionInfo":{"status":"ok","timestamp":1701013332693,"user_tz":-60,"elapsed":3,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"c45d93a3-826f-4cff-d2cb-fda083d68dc0"},"id":"4qGCN4Z1O_Xb","execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","execution_count":99,"id":"44d920e9","metadata":{"id":"44d920e9","executionInfo":{"status":"ok","timestamp":1701013332693,"user_tz":-60,"elapsed":2,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["epochs = 4  # This can be adjusted based on your dataset and model performance\n"]},{"cell_type":"code","execution_count":128,"id":"b63d17ae","metadata":{"id":"b63d17ae","executionInfo":{"status":"ok","timestamp":1701014860251,"user_tz":-60,"elapsed":378,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["def train(model, train_dataloader, optimizer, loss_fn, device):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0\n","\n","    for batch in train_dataloader:\n","        # Move batch to device\n","        input_ids = batch[0].to(device)\n","        attention_mask = batch[1].to(device)\n","        labels = batch[2].to(device)\n","\n","        # Clear previously calculated gradients\n","        model.zero_grad()\n","\n","        # Perform a forward pass. This will return logits.\n","        outputs = model(input_ids, attention_mask)\n","\n","        # Compute loss and accumulate the loss value\n","        loss = loss_fn(outputs, labels)\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate gradients\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","\n","    # Compute the average loss over the training data\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    return avg_train_loss\n","\n","def evaluate(model, validation_dataloader, loss_fn, device):\n","    model.eval()  # Set the model to evaluation mode\n","    total_loss = 0\n","\n","    for batch in validation_dataloader:\n","        # Move batch to device\n","        input_ids = batch[0].to(device)\n","        attention_mask = batch[1].to(device)\n","        labels = batch[2].to(device)\n","\n","        # Forward pass, calculate logits\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_mask)\n","\n","        # Compute loss and accumulate the loss value\n","        loss = loss_fn(outputs, labels)\n","        total_loss += loss.item()\n","\n","    avg_val_loss = total_loss / len(validation_dataloader)\n","\n","    return avg_val_loss\n"]},{"cell_type":"code","source":["# def train(model, train_dataloader, optimizer, loss_fn, device):\n","#     model.train()  # Set the model to training mode\n","#     total_loss = 0\n","\n","#     for batch in train_dataloader:\n","#         # Move batch to device\n","#         input_ids = batch[0].to(device)\n","#         attention_mask = batch[1].to(device)\n","#         labels = batch[2].to(device)\n","\n","#         # Clear previously calculated gradients\n","#         model.zero_grad()\n","\n","#         # Perform a forward pass. This will return a SequenceClassifierOutput object\n","#         outputs = model(input_ids, attention_mask=attention_mask)\n","\n","#         # Extract the logits from the SequenceClassifierOutput\n","#         logits = outputs.logits\n","\n","#         # Compute loss and accumulate the loss value\n","#         loss = loss_fn(logits, labels)\n","#         total_loss += loss.item()\n","\n","#         # Perform a backward pass to calculate gradients\n","#         loss.backward()\n","\n","#         # Update parameters\n","#         optimizer.step()\n","\n","#     # Compute the average loss over the training data\n","#     avg_train_loss = total_loss / len(train_dataloader)\n","\n","#     return avg_train_loss\n"],"metadata":{"id":"evQ7l10V9ybQ","executionInfo":{"status":"ok","timestamp":1701014392144,"user_tz":-60,"elapsed":266,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"evQ7l10V9ybQ","execution_count":120,"outputs":[]},{"cell_type":"code","source":["# def evaluate(model, validation_dataloader, loss_fn, device):\n","#     model.eval()  # Set the model to evaluation mode\n","#     total_loss = 0\n","\n","#     for batch in validation_dataloader:\n","#         # Move batch to device\n","#         input_ids = batch[0].to(device)\n","#         attention_mask = batch[1].to(device)\n","#         labels = batch[2].to(device)\n","\n","#         # Forward pass, calculate logits\n","#         with torch.no_grad():\n","#             outputs = model(input_ids, attention_mask=attention_mask)\n","\n","#             # Extract the logits\n","#             logits = outputs.logits\n","\n","#             # Compute loss and accumulate the loss value\n","#             loss = loss_fn(logits, labels)\n","#             total_loss += loss.item()\n","\n","#     avg_val_loss = total_loss / len(validation_dataloader)\n","\n","#     return avg_val_loss\n"],"metadata":{"id":"Xr1a-yeD91IV","executionInfo":{"status":"ok","timestamp":1701014858471,"user_tz":-60,"elapsed":308,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"Xr1a-yeD91IV","execution_count":127,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = '/content/drive/MyDrive/Levy/part1_checkpoint_basis'\n"],"metadata":{"id":"HqzFesLIR7wq","executionInfo":{"status":"ok","timestamp":1701014400171,"user_tz":-60,"elapsed":255,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"HqzFesLIR7wq","execution_count":122,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","def save_checkpoint(model, optimizer, epoch, filename):\n","    # Create the checkpoint directory if it doesn't exist\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","    # Construct the full path for the checkpoint\n","    checkpoint_path = os.path.join(checkpoint_dir, filename)\n","\n","    # Save the checkpoint\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict()\n","    }\n","    torch.save(checkpoint, checkpoint_path)\n","    print(f\"Checkpoint saved at {checkpoint_path}\")\n"],"metadata":{"id":"zCmSOnH5PD_h","executionInfo":{"status":"ok","timestamp":1701014400517,"user_tz":-60,"elapsed":1,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"zCmSOnH5PD_h","execution_count":123,"outputs":[]},{"cell_type":"code","execution_count":129,"id":"33510406","metadata":{"id":"33510406","outputId":"98650078-a422-4292-f1de-fd1c19390437","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701015795527,"user_tz":-60,"elapsed":933022,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","----------\n","Training loss: 0.6947590485967771\n","Validation loss: 0.6934443418184916\n","Checkpoint saved at /content/drive/MyDrive/Levy/part1_checkpoint_basis/model_checkpoint.pth\n","Checkpoint saved\n","Epoch 2/4\n","----------\n","Training loss: 0.691446015329072\n","Validation loss: 0.692858587106069\n","Checkpoint saved at /content/drive/MyDrive/Levy/part1_checkpoint_basis/model_checkpoint.pth\n","Checkpoint saved\n","Epoch 3/4\n","----------\n","Training loss: 0.6857012301583081\n","Validation loss: 0.6921091548601787\n","Checkpoint saved at /content/drive/MyDrive/Levy/part1_checkpoint_basis/model_checkpoint.pth\n","Checkpoint saved\n","Epoch 4/4\n","----------\n","Training loss: 0.6749370158320726\n","Validation loss: 0.696214169661204\n"]}],"source":["best_val_loss = float('inf')\n","\n","for epoch in range(epochs):\n","    print(f'Epoch {epoch + 1}/{epochs}')\n","    print('-' * 10)\n","\n","    train_loss = train(model, train_dataloader, optimizer, loss_fn, device)\n","    print(f'Training loss: {train_loss}')\n","\n","    val_loss = evaluate(model, test_dataloader, loss_fn, device)\n","    print(f'Validation loss: {val_loss}')\n","\n","    # Save checkpoint if validation loss has improved\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        save_checkpoint(model, optimizer, epoch, 'model_checkpoint.pth')\n","        print(\"Checkpoint saved\")\n"]},{"cell_type":"code","execution_count":106,"id":"6d5d5bf1","metadata":{"id":"6d5d5bf1","executionInfo":{"status":"ok","timestamp":1701014260462,"user_tz":-60,"elapsed":11,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"outputs":[],"source":["# def load_checkpoint(model, optimizer, filename):\n","#     checkpoint = torch.load(filename)\n","#     model.load_state_dict(checkpoint['model_state_dict'])\n","#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","#     return checkpoint['epoch']\n","\n","# # Before starting training, load the checkpoint if it exists\n","# checkpoint_filename = 'best_model_checkpoint.pth'\n","# try:\n","#     start_epoch = load_checkpoint(model, optimizer, checkpoint_filename) + 1\n","#     print(f\"Resuming training from epoch {start_epoch}\")\n","# except FileNotFoundError:\n","#     print(\"No checkpoint found, starting from scratch\")\n","#     start_epoch = 0\n","\n","# # Then, adjust your training loop to start from `start_epoch`\n","# for epoch in range(start_epoch, epochs):\n","#     # Training loop continues as before\n"]},{"cell_type":"code","source":["def predict(model, test_dataloader, device):\n","    model.eval()  # Set the model to evaluation mode\n","\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for batch in test_dataloader:\n","            input_ids = batch[0].to(device)\n","            attention_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","\n","            outputs = model(input_ids, attention_mask)\n","            logits = outputs\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = labels.to('cpu').numpy()\n","\n","            # Store predictions and true labels\n","            predictions.append(logits)\n","            true_labels.append(label_ids)\n","\n","    return predictions, true_labels\n"],"metadata":{"id":"Rc0Q_oWfPVwd","executionInfo":{"status":"ok","timestamp":1701014414760,"user_tz":-60,"elapsed":233,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"Rc0Q_oWfPVwd","execution_count":124,"outputs":[]},{"cell_type":"code","source":["# def predict(model, test_dataloader, device):\n","#     model.eval()  # Set the model to evaluation mode\n","\n","#     predictions = []\n","#     true_labels = []\n","\n","#     with torch.no_grad():\n","#         for batch in test_dataloader:\n","#             input_ids = batch[0].to(device)\n","#             attention_mask = batch[1].to(device)\n","#             labels = batch[2].to(device)\n","\n","#             # Perform a forward pass to get logits\n","#             outputs = model(input_ids, attention_mask=attention_mask)\n","\n","#             # Extract the logits from the SequenceClassifierOutput\n","#             logits = outputs.logits\n","\n","#             # Move logits and labels to CPU\n","#             logits = logits.detach().cpu().numpy()\n","#             label_ids = labels.to('cpu').numpy()\n","\n","#             # Store predictions and true labels\n","#             predictions.append(logits)\n","#             true_labels.append(label_ids)\n","\n","#     return predictions, true_labels\n"],"metadata":{"id":"3R7a1CEUCLkT","executionInfo":{"status":"ok","timestamp":1701014418467,"user_tz":-60,"elapsed":2,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"3R7a1CEUCLkT","execution_count":125,"outputs":[]},{"cell_type":"code","source":["predictions, true_labels = predict(model, test_dataloader, device)\n"],"metadata":{"id":"4XyFpDFnWEol","executionInfo":{"status":"ok","timestamp":1701015832712,"user_tz":-60,"elapsed":17880,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"4XyFpDFnWEol","execution_count":130,"outputs":[]},{"cell_type":"code","source":["type(predictions[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRSfM2ZOmS3l","executionInfo":{"status":"ok","timestamp":1701015832713,"user_tz":-60,"elapsed":6,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"7c471cdf-faf6-46fc-d459-5d6c42ca1858"},"id":"ZRSfM2ZOmS3l","execution_count":131,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import numpy as np\n","\n","# Flatten the outputs\n","flat_predictions = np.concatenate(predictions, axis=0)\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Convert logits to predicted class (0 or 1)\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n"],"metadata":{"id":"5euwgaeHWOaC","executionInfo":{"status":"ok","timestamp":1701015832713,"user_tz":-60,"elapsed":4,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"5euwgaeHWOaC","execution_count":132,"outputs":[]},{"cell_type":"code","source":["unique, counts = np.unique(flat_predictions, return_counts=True)\n","\n","# Combine unique values and counts into a dictionary for a similar effect to value_counts()\n","value_counts = dict(zip(unique, counts))\n","\n","print(value_counts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Su9emuLgmzNX","executionInfo":{"status":"ok","timestamp":1701015832713,"user_tz":-60,"elapsed":4,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"214ec890-1686-4c0d-ca9d-e066f46f2de5"},"id":"Su9emuLgmzNX","execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 365, 1: 228}\n"]}]},{"cell_type":"code","source":["accuracy = accuracy_score(flat_true_labels, flat_predictions)\n","precision, recall, f1, _ = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='binary')\n","\n","print(f'Accuracy: {accuracy}')\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n","print(f'F1 Score: {f1}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PPMj95EWTg7","executionInfo":{"status":"ok","timestamp":1701015832714,"user_tz":-60,"elapsed":3,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}},"outputId":"5fda7e3c-2a76-4a13-d587-45889b0716d4"},"id":"9PPMj95EWTg7","execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.5345699831365935\n","Precision: 0.5482456140350878\n","Recall: 0.41946308724832215\n","F1 Score: 0.4752851711026616\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1YGwqqlLWU9-","executionInfo":{"status":"ok","timestamp":1701009996266,"user_tz":-60,"elapsed":8,"user":{"displayName":"Familie van Kempen","userId":"13983334157849232851"}}},"id":"1YGwqqlLWU9-","execution_count":57,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}