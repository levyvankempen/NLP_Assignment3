{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d93598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3739d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv(\"data/processed_data_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810c3634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1675"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers['TEXT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221c5f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1532\n",
       "1    1431\n",
       "Name: cOPN, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['cOPN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f070cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f559e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/levyvankempen/miniconda/envs/torch-gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_length = 512  # BERT's maximum length\n",
    "\n",
    "# Tokenizing\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in answers['TEXT']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Text to encode\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_length,   # Pad & truncate\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attention masks\n",
    "                        return_tensors = 'pt',     # Return PyTorch tensors\n",
    "                        truncation=True,\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3a9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset for the extraversion trait (cEXT)\n",
    "X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(input_ids, answers['cEXT'], \n",
    "                                                                    random_state=42, test_size=0.2)\n",
    "\n",
    "# Do the same for attention masks\n",
    "train_masks_ext, test_masks_ext, _, _ = train_test_split(attention_masks, answers['cEXT'],\n",
    "                                                         random_state=42, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b704659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ext = y_train_ext.reset_index(drop=True)\n",
    "y_test_ext = y_test_ext.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717d56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ext = torch.tensor(y_train_ext)\n",
    "y_test_ext = torch.tensor(y_test_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2c882f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0dd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 8  # You can adjust batch size\n",
    "\n",
    "# Create the DataLoader for training set\n",
    "train_data = TensorDataset(X_train_ext, train_masks_ext, y_train_ext)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for test set\n",
    "test_data = TensorDataset(X_test_ext, test_masks_ext, y_test_ext)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f251ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        # Use the 'bert-base-uncased' pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Enhanced classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),  # Dropout for regularization\n",
    "            nn.Linear(768, 512),  # First layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Dropout(0.1),  # Additional dropout layer for regularization\n",
    "            nn.Linear(512, 128),  # Second layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Linear(128, 2)  # Final layer for binary classification\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass the inputs through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification\n",
    "        cls_output = outputs.pooler_output\n",
    "\n",
    "        # Pass the BERT output through the classifier\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed46a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertBinaryClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c0ab081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)  # Learning rate is adjustable\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac74f58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertBinaryClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866129e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4  # This can be adjusted based on your dataset and model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3634d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, loss_fn, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass. This will return logits.\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss and accumulate the loss value\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute the average loss over the training data\n",
    "    avg_train_loss = total_loss / len(train_dataloader)  \n",
    "    \n",
    "    return avg_train_loss\n",
    "\n",
    "def evaluate(model, validation_dataloader, loss_fn, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Forward pass, calculate logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss and accumulate the loss value\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(validation_dataloader)\n",
    "\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93118398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac3286d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n",
      "Training loss: 0.6926558505405079\n",
      "Validation loss: 0.6938677549362182\n",
      "Checkpoint saved\n",
      "Epoch 2/4\n",
      "----------\n",
      "Training loss: 0.6944544969584404\n",
      "Validation loss: 0.6932699282964071\n",
      "Checkpoint saved\n",
      "Epoch 3/4\n",
      "----------\n",
      "Training loss: 0.6931815976245637\n",
      "Validation loss: 0.6934319067001343\n",
      "Epoch 4/4\n",
      "----------\n",
      "Training loss: 0.6922610656982319\n",
      "Validation loss: 0.6996817016601562\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, device)\n",
    "    print(f'Training loss: {train_loss}')\n",
    "\n",
    "    val_loss = evaluate(model, test_dataloader, loss_fn, device)\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "\n",
    "    # Save checkpoint if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint(model, optimizer, epoch, 'best_model_checkpoint.pth')\n",
    "        print(\"Checkpoint saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "802da5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_checkpoint(model, optimizer, filename):\n",
    "#     checkpoint = torch.load(filename)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     return checkpoint['epoch']\n",
    "\n",
    "# # Before starting training, load the checkpoint if it exists\n",
    "# checkpoint_filename = 'best_model_checkpoint.pth'\n",
    "# try:\n",
    "#     start_epoch = load_checkpoint(model, optimizer, checkpoint_filename) + 1\n",
    "#     print(f\"Resuming training from epoch {start_epoch}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"No checkpoint found, starting from scratch\")\n",
    "#     start_epoch = 0\n",
    "\n",
    "# # Then, adjust your training loop to start from `start_epoch`\n",
    "# for epoch in range(start_epoch, epochs):\n",
    "#     # Training loop continues as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b749fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "\n",
    "    return predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040148a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = predict(model, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0e05d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Flatten the outputs\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Convert logits to predicted class (0 or 1)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2701c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5025295109612141\n",
      "Precision: 0.5025295109612141\n",
      "Recall: 1.0\n",
      "F1 Score: 0.6689113355780022\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='binary')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896a1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1a48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
