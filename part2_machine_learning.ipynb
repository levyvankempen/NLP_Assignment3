{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75790eb5",
   "metadata": {},
   "source": [
    "# Assignment 3 Natural Language Processing - Part 2: traditional machine learning approach: XGBoost\n",
    "\n",
    "After part 1 of the assignment, we concluded that the XGBoost performed better than the SVM regarding the traditional machine learning approaches. As such, we decided to use XGBoost as a machine learner for the remainder of the assignment.\n",
    "Furthermore, for the traditional methods we investigated TF-IDF, Part-of-Speech tagging and Bag-of-Words as input to the model. In part 1 we investigated which works better, but here we could not find any clear optimal solutions as all target variables had other methods that worked well. As such, we decided to test it again for one model predicting all 5 personality traits in one go, to see which input would work the best if we predicted everything in one go.\n",
    "\n",
    "Preprocessing was not changed as there were no significant changes and insights found in part 1 regarding the preprocessing.\n",
    "\n",
    "In the remainder of the assignment, we do no longer predict the classification (1 or 0) but continue using the probability of the text belonging to that class. We will thus have one model predicting the 5 classes in one go, with outcomes being the probability.\n",
    "\n",
    "This requires some changes in the model. We used XGBClassifier with the 'binary:hinge' objective function for the classification. To predict the probabilities, we need to convert the XGBClassifier into an XGBRegressor. Also, the objective function needs to be changed. The 'binary:hinge' function yields 1 or 0. We converted this to 'binary:logistic' to get outcome probabilities.\n",
    "To do the 5 classifications at once, we give the model as y variable the data frame containing all 5 target variables, instead of just one column as y.\n",
    "\n",
    "As we changed from binary classification to probability predictions, we decided to also test how a binary classifier would perform when predicting the 5 traits in one go. we will therefore also train and test 3 additional XGBClassifiers with the 'binary:hinge' objective function. This way we can identify if changing from binary classification to probability prediction has some implications for the performance. For parameters, we use the found optimal parameters when investigating the TF-IDF model of part 2.\n",
    "\n",
    "\n",
    "To assess the performance, we initially could not use F1, Accuracy, Precision and Recall anymore as these are solely for classification with categorical variables. The regressor yields numerical values which cannot be used for this. This would imply the usage of Regressor-based metrics such as RMSE. We however cannot compare the results with part 1 if using regressor-based metrics. Therefore, as we are predicting probabilities, we will convert the probability after prediction to 1 if the probability is equal/higher than 0.5 and to 0 if the prediction is lower than 0.5. This way, we can still create the desired performance metrics and compare them to other models. As we did find the F1 and AUC-ROC more important than Accuracy, Recall and Precision, we dropped the latter three and only looked at the F1 ROC-AUC\n",
    "\n",
    "For this part, we again test the three vectorization techniques separately. We first do a hyperparameter search to find the optimal XGBoost parameters. Here we changed some parameters to fit the XGBRegressor function instead of the XGBClassifier used for part 1. Continuing, we again validate the models and compare them to each other to see what vectorisation method works best. After this is done, we can investigate the capabilities of XGBoost for predicting the 5 traits in one go, as well as compare it to predicting the separate traits as done in part 1 and the deep learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0e5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedKFold\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cd72b",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0daa21ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well right woke midday nap Its sort weird ever...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well stream consciousness essay used thing lik...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>open keyboard button push The thing finally wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant believe Its really happening pulse racing...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well good old stream consciousness assignment ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>motivated day day basis need provide little fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>son biggest part life without reckless person ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>kid grandkids keep motivated everyday inspire ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>biggest drive earn money retire beach schedule...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>People never give cause life cruel strong enem...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2963 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   TEXT  cEXT  cNEU  cAGR  \\\n",
       "0     Well right woke midday nap Its sort weird ever...     0     1     1   \n",
       "1     Well stream consciousness essay used thing lik...     0     0     1   \n",
       "2     open keyboard button push The thing finally wo...     0     1     0   \n",
       "3     cant believe Its really happening pulse racing...     1     0     1   \n",
       "4     Well good old stream consciousness assignment ...     1     0     1   \n",
       "...                                                 ...   ...   ...   ...   \n",
       "2958  motivated day day basis need provide little fa...     1     0     0   \n",
       "2959  son biggest part life without reckless person ...     1     1     0   \n",
       "2960  kid grandkids keep motivated everyday inspire ...     1     0     1   \n",
       "2961  biggest drive earn money retire beach schedule...     0     0     0   \n",
       "2962  People never give cause life cruel strong enem...     1     1     0   \n",
       "\n",
       "      cCON  cOPN  \n",
       "0        0     1  \n",
       "1        0     0  \n",
       "2        1     1  \n",
       "3        1     0  \n",
       "4        0     1  \n",
       "...    ...   ...  \n",
       "2958     1     1  \n",
       "2959     0     0  \n",
       "2960     1     0  \n",
       "2961     0     0  \n",
       "2962     0     0  \n",
       "\n",
       "[2963 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_data_english_no_lowercasing.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a85d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['TEXT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d88e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd77f8",
   "metadata": {},
   "source": [
    "## Using TF-IDF \n",
    "\n",
    "First, train-test splits are made per target variable. Afterwards, X_train and X_test are transformed using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c30a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(corpus, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c640a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()  \n",
    "\n",
    "# for Ext\n",
    "X_train_t = tfidf.fit_transform(X_train_t)\n",
    "X_test_t = tfidf.transform(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613237b",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "In order to have the optimal XGBoost model, a Randomized Search is done to find the best combination of 'learning rate', 'gamma', 'lambda' and 'alpha' parameters of the XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f48a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ignore Sklearn warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbc82446",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV 1/2] END alpha=0.01, gamma=0.1, lambda=0.01, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.552) total time=  49.8s\n",
      "[CV 2/2] END alpha=0.01, gamma=0.1, lambda=0.01, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.556) total time=  54.7s\n",
      "[CV 1/2] END alpha=1.0, gamma=0.01, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.568) total time=  47.1s\n",
      "[CV 2/2] END alpha=1.0, gamma=0.01, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.565) total time=  46.1s\n",
      "[CV 1/2] END alpha=0.1, gamma=0.01, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.560) total time=  41.8s\n",
      "[CV 2/2] END alpha=0.1, gamma=0.01, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.556) total time=  38.6s\n",
      "[CV 1/2] END alpha=0.1, gamma=0.1, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.567) total time=  40.5s\n",
      "[CV 2/2] END alpha=0.1, gamma=0.1, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.556) total time=  40.5s\n",
      "[CV 1/2] END alpha=1.0, gamma=0.01, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.576) total time=  53.0s\n",
      "[CV 2/2] END alpha=1.0, gamma=0.01, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.559) total time=  55.4s\n",
      "[CV 1/2] END alpha=1.0, gamma=0.1, lambda=0.1, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.562) total time=  37.1s\n",
      "[CV 2/2] END alpha=1.0, gamma=0.1, lambda=0.1, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.558) total time=  37.6s\n",
      "[CV 1/2] END alpha=0.1, gamma=0.01, lambda=1.0, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.566) total time=  43.0s\n",
      "[CV 2/2] END alpha=0.1, gamma=0.01, lambda=1.0, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.557) total time=  42.2s\n",
      "[CV 1/2] END alpha=1.0, gamma=0.01, lambda=1.0, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.565) total time=  57.7s\n",
      "[CV 2/2] END alpha=1.0, gamma=0.01, lambda=1.0, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.563) total time=  54.1s\n",
      "[CV 1/2] END alpha=0.01, gamma=1.0, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.561) total time=  38.8s\n",
      "[CV 2/2] END alpha=0.01, gamma=1.0, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.561) total time=  35.7s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=1.0, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.560) total time=  55.0s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=1.0, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.566) total time=  55.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'lambda': 0.01, 'gamma': 0.1, 'alpha': 0.01}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective='binary:logistic')  # Set objective for regression\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "params_dict = {'learning_rate': [0.01, 0.1, 0.2],\n",
    "               'gamma': [0.01, 0.1, 1.0],\n",
    "               'lambda': [0.01, 0.1, 1.0],\n",
    "               'alpha': [0.01, 0.1, 1.0],\n",
    "              }\n",
    "\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score, average='micro'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', average='micro'),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=params_dict,\n",
    "    scoring=scoring,\n",
    "    refit='f1', \n",
    "    cv=2,\n",
    "    verbose=3)\n",
    "    \n",
    "search.fit(X_train_t, y_train_t)\n",
    "\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65a1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_r_t = {'learning_rate': 0.01, 'lambda': 0.01, 'gamma': 0.1, 'alpha': 0.01}\n",
    "opt_params_c_t = {'min_child_weight': 10, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5399e1",
   "metadata": {},
   "source": [
    "### Actual training and testing\n",
    "\n",
    "With the optimal parameters found, the XGBoost is trained on the training set and then tested for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8814d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_model_c(X_train, X_test, y_train, y_test, opt_params):\n",
    "    xgb_model = MultiOutputClassifier(xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=5,\n",
    "        min_child_weight=opt_params['min_child_weight'],\n",
    "        max_depth=opt_params['max_depth'],\n",
    "        learning_rate=opt_params['learning_rate'],\n",
    "        gamma=opt_params['gamma']\n",
    "    ))\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = xgb_model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test.values.flatten(), predictions.flatten(), average='micro')\n",
    "    roc_auc = roc_auc_score(y_test.values.flatten(), predictions.flatten(), average='micro')\n",
    "\n",
    "    print(\"F1 Score: {:.2f}\".format(f1))\n",
    "    print(\"ROC AUC: {:.2f}\".format(roc_auc))\n",
    "\n",
    "    return [f1, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28a13782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_model_r(X_train, X_test, y_train, y_test, opt_params):\n",
    "    xgb_model = xgb.XGBRegressor(objective='binary:logistic', \n",
    "                                reg_lambda=opt_params['lambda'], \n",
    "                                alpha=opt_params['alpha'], \n",
    "                                learning_rate=opt_params['learning_rate'], \n",
    "                                gamma=opt_params['gamma'],\n",
    "                                eval_metric='rmse')\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    predictions_prob = xgb_model.predict(X_test)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    predictions_binary = (predictions_prob > 0.5).astype(int)\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_test.values.flatten(), predictions_binary.flatten(), average='micro')\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test.values.flatten(), predictions_prob.flatten(), average='micro')\n",
    "    \n",
    "    \n",
    "    print(\"F1 Score: {:.2f}\".format(f1))\n",
    "    print(\"ROC AUC: {:.2f}\".format(roc_auc))\n",
    "\n",
    "    \n",
    "    \n",
    "    return [f1, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64ae2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.55\n",
      "ROC AUC: 0.55\n"
     ]
    }
   ],
   "source": [
    "# With binary classification\n",
    "metrics_t_c = XGB_model_c(X_train_t, X_test_t, y_train_t, y_test_t, opt_params_c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fe77a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.54\n",
      "ROC AUC: 0.56\n"
     ]
    }
   ],
   "source": [
    "# With probability outcomes\n",
    "metrics_t_r = XGB_model_r(X_train_t, X_test_t, y_train_t, y_test_t, opt_params_r_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cc39a",
   "metadata": {},
   "source": [
    "## Using Part-of-Speech (PoS) tagging\n",
    "\n",
    "As second method to test, PoS tagging as used. To do so, first the text inputs are tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbe41890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maxma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f65c0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df.copy()\n",
    "\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return [tag for word, tag in pos_tags]\n",
    "\n",
    "pos_df['pos_tags'] = pos_df['TEXT'].apply(pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd1a8b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxma\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "X_p = vectorizer.fit_transform(pos_df['pos_tags'])\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_p, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a388bcf",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "In order to have the optimal XGBoost model, a Randomized Search is done to find the best combination of 'learning rate', 'gamma', 'lambda' and 'alpha' parameters of the XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9eb4373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV 1/2] END alpha=1.0, gamma=1.0, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.524) total time=   0.1s\n",
      "[CV 2/2] END alpha=1.0, gamma=1.0, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.517) total time=   0.1s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.524) total time=   0.0s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.525) total time=   0.1s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.528) total time=   0.4s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.524) total time=   0.6s\n",
      "[CV 1/2] END alpha=0.1, gamma=0.1, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.518) total time=   0.5s\n",
      "[CV 2/2] END alpha=0.1, gamma=0.1, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.515) total time=   0.4s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.528) total time=   0.2s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.517) total time=   0.3s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=0.1, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.529) total time=   0.7s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=0.1, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.522) total time=   0.7s\n",
      "[CV 1/2] END alpha=0.01, gamma=0.1, lambda=1.0, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.520) total time=   0.4s\n",
      "[CV 2/2] END alpha=0.01, gamma=0.1, lambda=1.0, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.515) total time=   0.4s\n",
      "[CV 1/2] END alpha=0.01, gamma=0.01, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.520) total time=   0.4s\n",
      "[CV 2/2] END alpha=0.01, gamma=0.01, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.520) total time=   0.4s\n",
      "[CV 1/2] END alpha=1.0, gamma=1.0, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.526) total time=   0.2s\n",
      "[CV 2/2] END alpha=1.0, gamma=1.0, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.522) total time=   0.3s\n",
      "[CV 1/2] END alpha=0.01, gamma=0.1, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.521) total time=   0.6s\n",
      "[CV 2/2] END alpha=0.01, gamma=0.1, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.520) total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'lambda': 0.1, 'gamma': 1.0, 'alpha': 1.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective='binary:logistic')  # Set objective for regression\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "params_dict = {'learning_rate': [0.01, 0.1, 0.2],\n",
    "               'gamma': [0.01, 0.1, 1.0],\n",
    "               'lambda': [0.01, 0.1, 1.0],\n",
    "               'alpha': [0.01, 0.1, 1.0],\n",
    "              }\n",
    "\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score, average='micro'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', average='micro'),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=params_dict,\n",
    "    scoring=scoring,\n",
    "    refit='f1', \n",
    "    cv=2,\n",
    "    verbose=3)\n",
    "    \n",
    "search.fit(X_train_p, y_train_p)\n",
    "\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac6d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_r_p = {'learning_rate': 0.1, 'lambda': 0.1, 'gamma': 1.0, 'alpha': 1.0}\n",
    "opt_params_c_p = {'min_child_weight': 10, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76284c1c",
   "metadata": {},
   "source": [
    "### Actual training and testing\n",
    "\n",
    "With the optimal parameters found, the XGBoost is trained on the training set and then tested for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5579b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.52\n",
      "ROC AUC: 0.51\n"
     ]
    }
   ],
   "source": [
    "# With binary classification\n",
    "metrics_p_c = XGB_model_c(X_train_p, X_test_p, y_train_p, y_test_p, opt_params_c_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "735098d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.52\n",
      "ROC AUC: 0.52\n"
     ]
    }
   ],
   "source": [
    "# With probability outcomes\n",
    "metrics_p_r = XGB_model_r(X_train_p, X_test_p, y_train_p, y_test_p, opt_params_r_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19618738",
   "metadata": {},
   "source": [
    "## Using Bag-of-Words (BoW)\n",
    "\n",
    "Firstly Converts the texts into Bag-of-Words representation and determine the train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f5cb8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_b = vectorizer.fit_transform(df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af2d896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be94e4",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "In order to have the optimal XGBoost model, a Randomized Search is done to find the best combination of 'learning rate', 'gamma', 'lambda' and 'alpha' parameters of the XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f81595ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV 1/2] END alpha=1.0, gamma=1.0, lambda=0.1, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.559) total time=  11.5s\n",
      "[CV 2/2] END alpha=1.0, gamma=1.0, lambda=0.1, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.564) total time=  16.8s\n",
      "[CV 1/2] END alpha=1.0, gamma=0.1, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.566) total time=  17.4s\n",
      "[CV 2/2] END alpha=1.0, gamma=0.1, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.569) total time=  21.7s\n",
      "[CV 1/2] END alpha=1.0, gamma=1.0, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.562) total time=  11.9s\n",
      "[CV 2/2] END alpha=1.0, gamma=1.0, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.561) total time=  11.9s\n",
      "[CV 1/2] END alpha=0.01, gamma=1.0, lambda=0.01, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.554) total time=  18.9s\n",
      "[CV 2/2] END alpha=0.01, gamma=1.0, lambda=0.01, learning_rate=0.01; f1: (test=nan) roc_auc: (test=0.552) total time=  20.3s\n",
      "[CV 1/2] END alpha=0.1, gamma=0.1, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.556) total time=  15.2s\n",
      "[CV 2/2] END alpha=0.1, gamma=0.1, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.559) total time=  11.7s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.558) total time=   8.9s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=0.01, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.557) total time=   8.5s\n",
      "[CV 1/2] END alpha=0.1, gamma=0.01, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.558) total time=  14.3s\n",
      "[CV 2/2] END alpha=0.1, gamma=0.01, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.563) total time=  14.6s\n",
      "[CV 1/2] END alpha=0.1, gamma=1.0, lambda=1.0, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.561) total time=   8.7s\n",
      "[CV 2/2] END alpha=0.1, gamma=1.0, lambda=1.0, learning_rate=0.2; f1: (test=nan) roc_auc: (test=0.556) total time=   9.0s\n",
      "[CV 1/2] END alpha=0.01, gamma=1.0, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.558) total time=  13.3s\n",
      "[CV 2/2] END alpha=0.01, gamma=1.0, lambda=0.1, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.558) total time=  14.2s\n",
      "[CV 1/2] END alpha=0.01, gamma=0.01, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.566) total time=  14.9s\n",
      "[CV 2/2] END alpha=0.01, gamma=0.01, lambda=0.01, learning_rate=0.1; f1: (test=nan) roc_auc: (test=0.560) total time=  11.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'lambda': 0.1, 'gamma': 1.0, 'alpha': 1.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective='binary:logistic')  # Set objective for regression\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "params_dict = {'learning_rate': [0.01, 0.1, 0.2],\n",
    "               'gamma': [0.01, 0.1, 1.0],\n",
    "               'lambda': [0.01, 0.1, 1.0],\n",
    "               'alpha': [0.01, 0.1, 1.0],\n",
    "              }\n",
    "\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score, average='micro'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', average='micro'),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=params_dict,\n",
    "    scoring=scoring,\n",
    "    refit='f1', \n",
    "    cv=2,\n",
    "    verbose=3)\n",
    "    \n",
    "search.fit(X_train_b, y_train_b)\n",
    "\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c4a33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_r_b = {'learning_rate': 0.01, 'lambda': 0.1, 'gamma': 1.0, 'alpha': 1.0}\n",
    "opt_params_c_b = {'min_child_weight': 10, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173729f6",
   "metadata": {},
   "source": [
    "### Actual training and testing\n",
    "\n",
    "With the optimal parameters found, the XGBoost is trained on the training set and then tested for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "928765da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.54\n",
      "ROC AUC: 0.54\n"
     ]
    }
   ],
   "source": [
    "# With binary classification\n",
    "metrics_b_c = XGB_model_c(X_train_b, X_test_b, y_train_b, y_test_b, opt_params_c_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "385c3532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.53\n",
      "ROC AUC: 0.55\n"
     ]
    }
   ],
   "source": [
    "# With probability outcomes\n",
    "metrics_b_r = XGB_model_r(X_train_b, X_test_b, y_train_b, y_test_b, opt_params_r_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13358f07",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d1af86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.550422</td>\n",
       "      <td>0.550363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoS</th>\n",
       "      <td>0.515683</td>\n",
       "      <td>0.514978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoW</th>\n",
       "      <td>0.540304</td>\n",
       "      <td>0.540084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              F1   ROC-AUC\n",
       "TF-IDF  0.550422  0.550363\n",
       "PoS     0.515683  0.514978\n",
       "BoW     0.540304  0.540084"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary classification\n",
    "overall_metrics = pd.DataFrame([metrics_t_c,\n",
    "                                metrics_p_c,\n",
    "                                metrics_b_c],\n",
    "                                columns = ['F1', 'ROC-AUC'])\n",
    "\n",
    "new_index_values = ['TF-IDF', 'PoS', 'BoW']\n",
    "overall_metrics = overall_metrics.set_index(pd.Index(new_index_values))\n",
    "overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7578d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.537605</td>\n",
       "      <td>0.561213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoS</th>\n",
       "      <td>0.516020</td>\n",
       "      <td>0.516281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoW</th>\n",
       "      <td>0.530860</td>\n",
       "      <td>0.549288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              F1   ROC-AUC\n",
       "TF-IDF  0.537605  0.561213\n",
       "PoS     0.516020  0.516281\n",
       "BoW     0.530860  0.549288"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting probability\n",
    "overall_metrics = pd.DataFrame([metrics_t_r,\n",
    "                                metrics_p_r,\n",
    "                                metrics_b_r],\n",
    "                                columns = ['F1', 'ROC-AUC'])\n",
    "\n",
    "new_index_values = ['TF-IDF', 'PoS', 'BoW']\n",
    "overall_metrics = overall_metrics.set_index(pd.Index(new_index_values))\n",
    "overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097d893",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5bfe9",
   "metadata": {},
   "source": [
    "As in part one, we compared the 3 different vectorization techniques again as each personality trait preferred different techniques. We thus wanted to see if we could now identify a clear optimal technique. \n",
    "\n",
    "Inspecting the results, we can first identify that the F1 and ROC-AUC are relatively low. This can be said for all 3 vectorization techniques we tried. A machine learner clearly does not perform well when predicting 5 target variables at once. Comparing it to binary classification, we can also see a slight drop. Predicting the probabilities instead of binary classification thus makes it more difficult for the model to make predictions. Differences however are small and both prediction methods perform badly.\n",
    "\n",
    "\n",
    "## comparison to part 1\n",
    "In the following table, the F1 scores of part 1 and this part (both binary and probability) have been depicted.\n",
    "\n",
    "| Vectorisation technique             | Separate predicting | simultaneous predicting (binary) | simultaneous predicting (probability)\n",
    "| :----------: | :--: | :--: | :--: |\n",
    "| TF-IDF       |   0.661805   |  0.550422\t   | 0.537605 |\n",
    "| PoS           |   0.665579  |  0.515683\t    | 0.516020 |\n",
    "| BoW    |  0.664900   |  0.540304\t     | 0.530860 |\n",
    "\n",
    "From this table, it becomes clear that by shifting from separately predicting each target variable to all variables at once, the performance has become worse. The model clearly can predict each personality trait better if it is predicting it alone. Predicting all traits at once is more difficult. This finding makes sense as it now has to consider 5 targets instead of 1, which might imply an increased complexity and interdependence of the personality traits. It thus is better to train a model for each trait separately.\n",
    "\n",
    "Concludingly, the performance of predicting everything in one go is not high. It does not matter if the prediction is made with binary classification or with probabilities as both perform almost identically. Compared to part 1, the performance dropped so that a machine learner can better predict the traits separately than all in one go.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
